{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Based Sentiment Analysis on Movie Reviews\n",
    "\n",
    "Hi, guys. So I am trying on reproducing the technique discussed in this paper[1]. The process involves no machine learning, and consists of: tokenizing reviews into sentences, POS Tagging, identifying aspects, locating opinion word of aspects, and calculating opinion polarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv,re,sys\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# set positive or negative based on arguments\n",
    "\n",
    "review_type = \"pos\"\n",
    "if len(sys.argv) > 1 and sys.argv[1] in [\"pos\", \"neg\"]:\n",
    "\treview_type = sys.argv[1]\n",
    "\n",
    "input_files = ['../dataset/review_test_'+review_type+'.csv']\n",
    "items = [];\n",
    "csvwriter = csv.writer(open(\"fullsen_\"+review_type+\".csv\",'w'), delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "for input_file in input_files:\n",
    "\treadCsv = csv.reader(open(input_file))\n",
    "\tfor row in readCsv:\n",
    "\t\treviewId = row[0]\n",
    "\t\treviewText = re.sub('<br />',' ',row[1])\n",
    "\t\tmovieId = row[3]\n",
    "\t\ttry:\n",
    "\t\t\tsentences = sent_tokenize(reviewText)\n",
    "\t\t\tfor idx, sentence in enumerate(sentences):\n",
    "\t\t\t\titems.append([movieId, reviewId, idx, sentence])\n",
    "\t\t\t\tcsvwriter.writerow([movieId, reviewId, idx, sentence])\n",
    "\t\texcept:\n",
    "\t\t\tcontinue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all sentences\n",
    "review_type = \"pos\"\n",
    "input_files = ['../python-port/fullsen_'+review_type+'.csv']\n",
    "sentences = []\n",
    "\n",
    "for input_file in input_files:\n",
    "\treadCsv = csv.reader(open(input_file))\n",
    "\tfor row in readCsv:\n",
    "\t\tsentence = {\"movieId\": row[0], \"reviewId\": row[1], \"sentenceId\": row[2], \"text\":row[3]}\n",
    "\t\tsentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample sentence\n",
    "sentence = sentences[8526]\n",
    "# apply pos tagging\n",
    "import nltk\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence[\"text\"])\n",
    "    sentence[\"pos_tag\"] = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Aspects\n",
    "\n",
    "I used the terms list on [2] (see Table 2) and the movie metadata (movie title, actor names, character names, director names) to identify the aspect. The feature terms is expanded using WordNet similarity (threshold= ... ), so whenever we found NP (nounphrase) in the sentence, we match it to its WordNet counterpart using WSD (word-sense disambiguation) and calculate the average similarity to the already available terms.\n",
    "\n",
    "Aspect Categories:\n",
    "* Overall\n",
    "* Cast\n",
    "* Director\n",
    "* Story\n",
    "* Scene / Cinematography\n",
    "* Music\n",
    "\n",
    "First we built a aspect category dictionary consisting of related word-sense (it's taken from Table 2 of [2], and manually checked the appropriate word sense via [this tool](http://wordnetweb.princeton.edu/perl/webwn). For now we will start only with 'overall' and 'cast' aspect category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "aspect_terms = {\"overall\": wn.synset('movie.n.01'), wn.synset('film.n.01'),\n",
    "\"cast\": wn.synset('act.v.10'), wn.synset('act.n.04'), wn.synset('acting.n.01'), wn.synset('actress.n.01'), wn.synset('actor.n.01'), wn.synset('role.n.02'), wn.synset('portray.n.03'), wn.synset('character.n.04'), wn.synset('villain.n.02'), wn.synset('performance.n.02'), wn.synset('performed.v.03'), wn.synset('played.v.25'), wn.sysnet('casting.n.04'), wn.synset('cast.n.01'), wn.synset('cast.v.03')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next for each sentence, we extracted the verbs and noun, apply word-sense disambiguation to determine the word sense, and check if it exists in `aspect_terms`. We can disregard and skip stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.utils import penn2morphy\n",
    "\n",
    "def is_stopword(string):\n",
    "    if string.lower() in nltk.corpus.stopwords.words('english'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_punctuation(string):\n",
    "    for char in string:\n",
    "        if char.isalpha() or char.isdigit():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    for item in sentence[\"pos_tag\"]:\n",
    "        token = item[0]\n",
    "        pos = item[1]\n",
    "        if ~is_stopword(token) and ~is_punctuation(token):\n",
    "            if pos.startswith('NN') or pos.startswith('VB'): # it is a noun\n",
    "                word_sense = simple_lesk()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locating Opinion Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Opinion Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1]: [Piryani, R., Gupta, V., Singh, V. K., & Ghose, U. (2017). A Linguistic Rule-Based Approach for Aspect-Level Sentiment Analysis of Movie Reviews. In Advances in Computer and Computational Sciences (pp. 201-209). Springer, Singapore.](https://link.springer.com/chapter/10.1007/978-981-10-3770-2_19)\n",
    "[2]: [Thet, T. T., Na, J. C., & Khoo, C. S. (2010). Aspect-based sentiment analysis of movie reviews on discussion boards. Journal of information science, 36(6), 823-848.](https://dl.acm.org/citation.cfm?id=1899344)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
